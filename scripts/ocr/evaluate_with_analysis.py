"""
ğŸ“Š Script de AvaliaÃ§Ã£o de OCR com AnÃ¡lise Detalhada
Executa avaliaÃ§Ã£o de engine OCR e gera anÃ¡lise completa com grÃ¡ficos e estatÃ­sticas.

Uso:
    python scripts/ocr/evaluate_with_analysis.py --engine parseq_enhanced --test-data data/ocr_test --output outputs/ocr_analysis
"""

import argparse
import sys
from pathlib import Path

import cv2
from loguru import logger

# Adicionar src ao path
SCRIPT_DIR = Path(__file__).resolve().parent
ROOT_DIR = SCRIPT_DIR.parent.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from src.ocr.evaluator import OCREvaluator


def parse_args():
    parser = argparse.ArgumentParser(
        description="AvaliaÃ§Ã£o de OCR com anÃ¡lise detalhada"
    )
    parser.add_argument(
        "--engine",
        type=str,
        required=True,
        help="Nome do engine (tesseract, easyocr, paddleocr, parseq, parseq_enhanced, trocr)"
    )
    parser.add_argument(
        "--config",
        type=str,
        help="Caminho para arquivo de configuraÃ§Ã£o do engine"
    )
    parser.add_argument(
        "--test-data",
        type=str,
        required=True,
        help="DiretÃ³rio com dados de teste (deve conter ground_truth.json e pasta images/)"
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="DiretÃ³rio de saÃ­da para anÃ¡lise"
    )
    parser.add_argument(
        "--preprocessing",
        type=str,
        help="ConfiguraÃ§Ã£o de prÃ©-processamento (ppro-tesseract, ppro-parseq, etc.)"
    )
    parser.add_argument(
        "--skip-plots",
        action="store_true",
        help="Pular geraÃ§Ã£o de grÃ¡ficos (Ãºtil se matplotlib nÃ£o estiver instalado)"
    )
    
    return parser.parse_args()


def main():
    args = parse_args()
    
    logger.info("="*80)
    logger.info("ğŸ“Š AVALIAÃ‡ÃƒO DE OCR COM ANÃLISE DETALHADA")
    logger.info("="*80)
    logger.info(f"ğŸ¤– Engine: {args.engine}")
    logger.info(f"ğŸ“ Dados de teste: {args.test_data}")
    logger.info(f"ğŸ’¾ SaÃ­da: {args.output}")
    logger.info("="*80)
    
    # Criar evaluator
    evaluator = OCREvaluator()
    
    # Buscar configuraÃ§Ã£o do engine
    if args.config:
        engine_config_path = args.config
    else:
        # Buscar configuraÃ§Ã£o padrÃ£o
        config_dir = Path(__file__).parent.parent.parent / 'config' / 'ocr'
        engine_config_path = config_dir / f'{args.engine}.yaml'
        
        if not engine_config_path.exists():
            logger.warning(f"âš ï¸ ConfiguraÃ§Ã£o nÃ£o encontrada: {engine_config_path}")
            engine_config_path = None
    
    # Adicionar engine
    try:
        evaluator.add_engine(args.engine, str(engine_config_path) if engine_config_path else None)
    except Exception as e:
        logger.error(f"âŒ Erro ao adicionar engine: {e}")
        return 1
    
    # Verificar dados de teste
    test_data_path = Path(args.test_data)
    if not test_data_path.exists():
        logger.error(f"âŒ DiretÃ³rio de teste nÃ£o encontrado: {test_data_path}")
        return 1
    
    gt_path = test_data_path / "ground_truth.json"
    if not gt_path.exists():
        logger.error(f"âŒ Ground truth nÃ£o encontrado: {gt_path}")
        logger.info("ğŸ’¡ Execute: make ocr-annotate")
        return 1
    
    images_dir = test_data_path / "images"
    if not images_dir.exists():
        logger.error(f"âŒ DiretÃ³rio de imagens nÃ£o encontrado: {images_dir}")
        return 1
    
    # Carregar ground truth
    import json
    with open(gt_path, 'r', encoding='utf-8') as f:
        gt_data = json.load(f)
    
    if 'annotations' not in gt_data:
        logger.error("âŒ Ground truth invÃ¡lido (falta campo 'annotations')")
        return 1
    
    ground_truth = gt_data['annotations']
    
    # Carregar preprocessador se especificado
    preprocessor = None
    if args.preprocessing:
        from src.ocr.config import load_preprocessing_config
        from src.ocr.preprocessors import ImagePreprocessor
        
        preprocessing_name = args.preprocessing if args.preprocessing.startswith('ppro-') else f'ppro-{args.preprocessing}'
        config_dir = Path(__file__).parent.parent.parent / 'config' / 'preprocessing'
        prep_config_path = config_dir / f'{preprocessing_name}.yaml'
        
        if prep_config_path.exists():
            try:
                prep_config = load_preprocessing_config(str(prep_config_path))
                preprocessor = ImagePreprocessor(prep_config)
                logger.success(f"âœ… PrÃ©-processamento configurado: {preprocessing_name}")
            except Exception as e:
                logger.warning(f"âš ï¸ Erro ao carregar prÃ©-processamento: {e}")
        else:
            logger.warning(f"âš ï¸ ConfiguraÃ§Ã£o de prÃ©-processamento nÃ£o encontrada: {prep_config_path}")
    
    # Processar imagens
    image_files = list(images_dir.glob("*.jpg")) + list(images_dir.glob("*.png"))
    
    if not image_files:
        logger.error(f"âŒ Nenhuma imagem encontrada em: {images_dir}")
        return 1
    
    logger.info(f"ğŸ“¸ Processando {len(image_files)} imagens...")
    
    # Criar diretÃ³rio para debug de imagens
    debug_dir = output_dir / "debug_images"
    debug_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"ğŸ–¼ï¸  Salvando imagens de debug em: {debug_dir}")
    
    results = []
    for img_file in image_files:
        filename = img_file.name
        
        # Verificar ground truth
        if filename not in ground_truth:
            logger.warning(f"âš ï¸ Ground truth nÃ£o encontrado para: {filename}")
            continue
        
        expected_text = ground_truth[filename]
        
        if not expected_text or expected_text.strip() == "":
            logger.warning(f"âš ï¸ Ground truth vazio para: {filename}")
            continue
        
        # Carregar imagem
        image = cv2.imread(str(img_file))
        if image is None:
            logger.warning(f"âš ï¸ Erro ao carregar: {filename}")
            continue
        
        # Salvar imagem original
        img_debug_dir = debug_dir / Path(filename).stem
        img_debug_dir.mkdir(exist_ok=True)
        cv2.imwrite(str(img_debug_dir / "00_original.png"), image)
        
        # Aplicar prÃ©-processamento e salvar etapas
        if preprocessor:
            try:
                preprocessed = preprocessor.process(image)
                cv2.imwrite(str(img_debug_dir / "01_preprocessed.png"), preprocessed)
            except Exception as e:
                logger.warning(f"âš ï¸ Erro no prÃ©-processamento: {e}")
                preprocessed = image
        else:
            preprocessed = image
        
        # Avaliar
        try:
            result = evaluator.evaluate_single(
                image,
                expected_text,
                args.engine,
                preprocessor=preprocessor,
                debug_dir=img_debug_dir  # Passar diretÃ³rio de debug
            )
            result['image_file'] = filename
            results.append(result)
            
            # Log progresso
            status = 'âœ…' if result['exact_match'] else 'âŒ'
            logger.info(f"  {status} {filename}: '{result.get('predicted_text', '')}' vs '{expected_text}' (CER: {result['character_error_rate']:.3f})")
            
        except Exception as e:
            logger.error(f"âŒ Erro ao processar {filename}: {e}")
            import traceback
            logger.debug(traceback.format_exc())
    
    if not results:
        logger.error("âŒ Nenhum resultado obtido")
        return 1
    
    # Converter para DataFrame
    import pandas as pd
    df = pd.DataFrame(results)
    
    # Criar diretÃ³rio de saÃ­da
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Salvar resultados bÃ¡sicos
    results_file = output_dir / f"{args.engine}_results.csv"
    df.to_csv(results_file, index=False, encoding='utf-8')
    logger.success(f"ğŸ’¾ Resultados salvos: {results_file}")
    
    # Salvar JSON tambÃ©m
    json_file = output_dir / f"{args.engine}_results.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    logger.success(f"ğŸ’¾ Resultados JSON salvos: {json_file}")
    
    # Gerar anÃ¡lise detalhada com visualizaÃ§Ãµes
    logger.info("\n" + "="*80)
    logger.info("ğŸ¨ GERANDO ANÃLISE DETALHADA")
    logger.info("="*80)
    
    try:
        stats = evaluator.generate_detailed_analysis(df, str(output_dir))
        
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š RESUMO DA ANÃLISE")
        logger.info("="*80)
        
        basic = stats.get('basic', {})
        logger.info(f"âœ… Total de amostras: {basic.get('total_samples', 0)}")
        logger.info(f"âœ… Exact Match Rate: {basic.get('exact_match_rate', 0):.2%}")
        logger.info(f"ğŸ“Š Average CER: {basic.get('avg_cer', 0):.3f}")
        logger.info(f"ğŸ“Š Median CER: {basic.get('median_cer', 0):.3f}")
        logger.info(f"ğŸ“ˆ Average Confidence: {basic.get('avg_confidence', 0):.2%}")
        logger.info(f"â±ï¸  Average Time: {basic.get('avg_processing_time', 0):.3f}s")
        logger.info(f"â±ï¸  Total Time: {basic.get('total_processing_time', 0):.1f}s")
        
        # Categorias de erro
        errors = stats.get('errors', {})
        if errors:
            logger.info("\nğŸ“Š Categorias de Erro:")
            for category, data in errors.items():
                if isinstance(data, dict) and 'count' in data:
                    logger.info(f"  â€¢ {category}: {data['count']} ({data.get('percentage', 0):.1f}%)")
        
        logger.info("\n" + "="*80)
        logger.info("âœ… ANÃLISE COMPLETA!")
        logger.info("="*80)
        logger.info(f"ğŸ“ Todos os arquivos foram salvos em: {output_dir}")
        logger.info(f"ğŸ“„ RelatÃ³rio HTML: {output_dir / 'report.html'}")
        logger.info(f"ğŸ“Š EstatÃ­sticas JSON: {output_dir / 'statistics.json'}")
        logger.info("="*80)
        
    except Exception as e:
        logger.error(f"âŒ Erro ao gerar anÃ¡lise detalhada: {e}")
        import traceback
        traceback.print_exc()
        
        # Exibir resumo bÃ¡sico mesmo com erro
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š RESUMO BÃSICO")
        logger.info("="*80)
        
        exact_matches = sum(1 for r in results if r.get('exact_match', False))
        total = len(results)
        accuracy = exact_matches / total if total > 0 else 0
        
        import numpy as np
        avg_time = np.mean([r.get('processing_time', 0) for r in results])
        avg_conf = np.mean([r.get('confidence', 0) for r in results if r.get('confidence', 0) > 0])
        avg_cer = np.mean([r.get('character_error_rate', 0) for r in results])
        
        logger.info(f"âœ… Exact Match: {exact_matches}/{total} ({accuracy*100:.1f}%)")
        logger.info(f"ğŸ“Š Average CER: {avg_cer:.3f}")
        logger.info(f"â±ï¸  Tempo mÃ©dio: {avg_time:.3f}s")
        logger.info(f"ğŸ“ˆ ConfianÃ§a mÃ©dia: {avg_conf:.2f}")
        logger.info("="*80)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
