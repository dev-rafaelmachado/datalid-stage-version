"""
üìä Avalia√ß√£o Completa da Pipeline
Avalia pipeline end-to-end (YOLO ‚Üí OCR ‚Üí Parsing) com m√©tricas detalhadas.

Funcionalidades:
- Avalia√ß√£o em m√∫ltiplas imagens (configur√°vel)
- M√©tricas por etapa (detec√ß√£o, OCR, parsing)
- M√©tricas end-to-end
- An√°lise de erros detalhada
- Visualiza√ß√µes e relat√≥rios
- Compara√ß√£o com ground truth

Uso:
    # Avaliar com configura√ß√£o padr√£o
    python scripts/pipeline/evaluate_pipeline.py
    
    # Com config customizada
    python scripts/pipeline/evaluate_pipeline.py --config config/pipeline/pipeline_evaluation.yaml
    
    # Avaliar apenas 10 imagens
    python scripts/pipeline/evaluate_pipeline.py --num-images 10
    
    # Modo de sele√ß√£o aleat√≥ria
    python scripts/pipeline/evaluate_pipeline.py --num-images 20 --selection-mode random
    
    # Output customizado
    python scripts/pipeline/evaluate_pipeline.py --output outputs/minha_avaliacao
"""

import argparse
import json
import sys
import time
from datetime import datetime
from difflib import SequenceMatcher
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import cv2
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import yaml
from loguru import logger
from tqdm import tqdm

# Adicionar src ao path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.pipeline.full_pipeline import FullPipeline, load_pipeline_config


class PipelineEvaluator:
    """Avaliador completo da pipeline."""
    
    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa o avaliador.
        
        Args:
            config: Configura√ß√£o da avalia√ß√£o
        """
        self.config = config
        self.pipeline = None
        self.results = []
        self.metrics_summary = {}
        self.error_analysis = {}
        
        # Para salvar imagens de debug
        self.debug_images_count = 0
        self.max_debug_images = 3  # Salvar apenas 3 exemplos
        self.debug_output_dir = None
        
    def initialize_pipeline(self):
        """Inicializa a pipeline."""
        logger.info("üîÑ Inicializando pipeline...")
        
        pipeline_config_path = self.config['pipeline']['config_file']
        pipeline_config = load_pipeline_config(pipeline_config_path)
        
        self.pipeline = FullPipeline(pipeline_config)
        
        logger.info("‚úÖ Pipeline inicializada!")
        
    def load_ground_truth(self) -> Dict[str, str]:
        """
        Carrega ground truth e extrai expiry_date por imagem.
        
        Suporta dois formatos:
        1. Formato antigo: {"annotations": {"image.jpg": "text", ...}}
        2. Formato novo: {"detection_id": {"image": "image.jpg", "expiry_date": "...", ...}, ...}
        
        Returns:
            Dict mapeando nome da imagem para expiry_date esperado
        """
        gt_path = Path(self.config['dataset']['ground_truth'])
        
        if not gt_path.exists():
            raise FileNotFoundError(f"Ground truth n√£o encontrado: {gt_path}")
        
        logger.info(f"üì• Carregando ground truth: {gt_path}")
        
        with open(gt_path, 'r', encoding='utf-8') as f:
            gt_data = json.load(f)
        
        # Extrair anota√ß√µes baseado no formato
        json_key = self.config['validation']['ground_truth_format']['json_key']
        field_name = self.config['validation']['ground_truth_format'].get('field_name', 'expiry_date')
        
        annotations = {}
        
        # Tenta o formato antigo primeiro
        if json_key in gt_data:
            raw_annotations = gt_data[json_key]
            # Se for o formato simples (image -> text)
            if isinstance(next(iter(raw_annotations.values()), None), str):
                annotations = raw_annotations
            else:
                # Formato nested
                for key, value in raw_annotations.items():
                    if isinstance(value, dict) and 'image' in value:
                        image_name = value['image']
                        expiry = value.get(field_name, '')
                        annotations[image_name] = expiry
        else:
            # Formato novo direto: cada chave √© uma detec√ß√£o
            for detection_id, detection_data in gt_data.items():
                if isinstance(detection_data, dict) and 'image' in detection_data:
                    image_name = detection_data['image']
                    expiry = detection_data.get(field_name, '')
                    # Pega a primeira detec√ß√£o de cada imagem
                    if image_name not in annotations:
                        annotations[image_name] = expiry
        
        logger.info(f"‚úÖ Ground truth carregado: {len(annotations)} anota√ß√µes")
        
        return annotations
    
    def _extract_image_number(self, filename: str) -> Optional[str]:
        """
        Extrai o n√∫mero identificador de uma imagem.
        
        Exemplo: "train_101_jpg_6fee8d46fda6271dea968494c8a2c663.jpg" -> "101"
        
        Args:
            filename: Nome do arquivo ou ID da imagem
            
        Returns:
            N√∫mero extra√≠do ou None se n√£o encontrado
        """
        import re

        # Procura por padr√£o _XXX_ onde XXX s√£o d√≠gitos
        match = re.search(r'_(\d+)_', filename)
        if match:
            return match.group(1)
        # Fallback: procura qualquer sequ√™ncia de d√≠gitos
        match = re.search(r'(\d+)', filename)
        if match:
            return match.group(1)
        return None
    
    def _build_image_mapping(self, annotations: Dict[str, str]) -> Dict[str, Tuple[str, str]]:
        """
        Cria mapeamento entre n√∫meros de imagens e arquivos reais.
        
        Args:
            annotations: Dicion√°rio com anota√ß√µes (keys s√£o IDs do ground truth)
            
        Returns:
            Dict mapeando n√∫mero da imagem -> (ground_truth_id, arquivo_real, expiry_date)
        """
        images_dir = Path(self.config['dataset']['images_dir'])
        
        # Mapear n√∫meros das anota√ß√µes
        gt_number_to_id = {}
        for gt_id, expiry_date in annotations.items():
            number = self._extract_image_number(gt_id)
            if number:
                gt_number_to_id[number] = (gt_id, expiry_date)
                logger.debug(f"  GT: '{gt_id}' -> n√∫mero '{number}' -> expiry '{expiry_date}'")
        
        # Mapear n√∫meros dos arquivos reais
        real_files = list(images_dir.glob('*.jpg')) + list(images_dir.glob('*.jpeg')) + list(images_dir.glob('*.png'))
        number_to_file = {}
        for file_path in real_files:
            number = self._extract_image_number(file_path.name)
            if number:
                number_to_file[number] = file_path.name
                logger.debug(f"  Arquivo: '{file_path.name}' -> n√∫mero '{number}'")
        
        # Criar mapeamento final: n√∫mero -> (gt_id, arquivo_real, expiry_date)
        mapping = {}
        matched_numbers = []
        unmatched_gt = []
        unmatched_files = []
        
        for number, (gt_id, expiry_date) in gt_number_to_id.items():
            if number in number_to_file:
                mapping[number] = (gt_id, number_to_file[number], expiry_date)
                matched_numbers.append(number)
                logger.debug(f"  ‚úÖ Match #{number}: GT '{gt_id}' <-> File '{number_to_file[number]}'")
            else:
                unmatched_gt.append((number, gt_id))
                logger.warning(f"  ‚ùå N√∫mero {number} do ground truth '{gt_id}' n√£o encontrado nos arquivos")
        
        # Arquivos sem match no GT
        for number, filename in number_to_file.items():
            if number not in gt_number_to_id:
                unmatched_files.append((number, filename))
        
        # Relat√≥rio de mapeamento
        logger.info(f"üîó Mapeamento criado:")
        logger.info(f"   üìù Ground truth entries: {len(annotations)}")
        logger.info(f"   üìÅ Arquivos encontrados: {len(real_files)}")
        logger.info(f"   ‚úÖ Matches bem-sucedidos: {len(mapping)}")
        
        if unmatched_gt:
            logger.warning(f"   ‚ö†Ô∏è  GT sem arquivo correspondente: {len(unmatched_gt)}")
            for num, gt_id in unmatched_gt[:3]:  # Mostra apenas os 3 primeiros
                logger.warning(f"      - #{num}: {gt_id}")
            if len(unmatched_gt) > 3:
                logger.warning(f"      ... e mais {len(unmatched_gt) - 3}")
        
        if unmatched_files:
            logger.info(f"   ‚ÑπÔ∏è  Arquivos sem GT: {len(unmatched_files)}")
            for num, filename in unmatched_files[:3]:
                logger.info(f"      - #{num}: {filename}")
            if len(unmatched_files) > 3:
                logger.info(f"      ... e mais {len(unmatched_files) - 3}")
        
        return mapping
    
    def select_images(self, annotations: Dict[str, str]) -> List[Tuple[str, str, str]]:
        """
        Seleciona imagens para avaliar e faz matching com arquivos reais.
        
        Args:
            annotations: Dicion√°rio com anota√ß√µes
            
        Returns:
            Lista de tuplas (n√∫mero, arquivo_real, expiry_date)
        """
        # Criar mapeamento entre ground truth e arquivos reais
        mapping = self._build_image_mapping(annotations)
        
        if not mapping:
            logger.error("‚ùå Nenhuma imagem foi mapeada com sucesso!")
            return []
        
        all_numbers = list(mapping.keys())
        num_images = self.config['dataset'].get('num_images')
        selection_mode = self.config['dataset'].get('selection_mode', 'random')
        
        # Se num_images for None, usar todas
        if num_images is None or selection_mode == 'all':
            selected_numbers = all_numbers
        elif selection_mode == 'first':
            selected_numbers = sorted(all_numbers, key=lambda x: int(x))[:num_images]
        elif selection_mode == 'random':
            import random
            random.seed(self.config['dataset'].get('random_seed', 42))
            selected_numbers = random.sample(all_numbers, min(num_images, len(all_numbers)))
        else:
            logger.warning(f"‚ö†Ô∏è Modo de sele√ß√£o inv√°lido: {selection_mode}, usando 'all'")
            selected_numbers = all_numbers
        
        # Converter para lista de tuplas (n√∫mero, arquivo_real, expiry_date)
        selected = [(num, mapping[num][1], mapping[num][2]) for num in selected_numbers]
        
        logger.info(f"üì∏ Imagens selecionadas: {len(selected)}/{len(all_numbers)}")
        logger.info(f"   Modo: {selection_mode}")
        
        return selected
    
    def evaluate(self) -> Dict[str, Any]:
        """
        Executa avalia√ß√£o completa.
        
        Returns:
            Dicion√°rio com resultados e m√©tricas
        """
        logger.info("=" * 80)
        logger.info("üìä INICIANDO AVALIA√á√ÉO DA PIPELINE")
        logger.info("=" * 80)
        
        # Carregar ground truth
        annotations = self.load_ground_truth()
        
        # Selecionar imagens
        selected_images = self.select_images(annotations)
        
        # Diret√≥rio de imagens
        images_dir = Path(self.config['dataset']['images_dir'])
        
        # Processar cada imagem
        logger.info(f"\nüîÑ Processando {len(selected_images)} imagens...")
        
        for number, real_filename, expiry_date in tqdm(selected_images, desc="Avaliando"):
            img_path = images_dir / real_filename
            
            if not img_path.exists():
                logger.warning(f"‚ö†Ô∏è Imagem n√£o encontrada: {real_filename} (n√∫mero: {number})")
                continue
            
            # Processar imagem com o arquivo real e expiry_date correto
            result = self._evaluate_single_image(
                img_path,
                real_filename,
                expiry_date
            )
            # Adicionar n√∫mero da imagem ao resultado para rastreamento
            result['image_number'] = number
            
            self.results.append(result)
        
        # Calcular m√©tricas agregadas
        logger.info("\nüìä Calculando m√©tricas agregadas...")
        self.metrics_summary = self._calculate_metrics_summary()
        
        # An√°lise de erros
        if self.config['error_analysis']['enabled']:
            logger.info("\nüîç Analisando erros...")
            self.error_analysis = self._analyze_errors()
        
        return {
            'results': self.results,
            'metrics_summary': self.metrics_summary,
            'error_analysis': self.error_analysis
        }
    
    def _evaluate_single_image(
        self,
        img_path: Path,
        img_name: str,
        ground_truth: str
    ) -> Dict[str, Any]:
        """
        Avalia uma √∫nica imagem.
        
        Args:
            img_path: Caminho da imagem
            img_name: Nome da imagem
            ground_truth: Texto esperado (ground truth)
            
        Returns:
            Dicion√°rio com resultados
        """
        result = {
            'image_name': img_name,
            'image_path': str(img_path),
            'ground_truth': ground_truth,
            'success': False,
            'error': None,
            # Inicializar todos os campos para evitar KeyError
            'processing_time': 0.0,
            'num_detections': 0,
            'detection_confidences': [],
            'avg_detection_confidence': 0.0,
            'ocr_texts': [],
            'ocr_confidences': [],
            'num_dates_found': 0,
            'dates': [],
            'predicted_text': '',
            'predicted_date': '',
            'final_confidence': 0.0,
            'exact_match': False,
            'partial_match': False,
            'similarity': 0.0,
            'cer': 1.0,
            'wer': 1.0
        }
        
        try:
            # Carregar imagem
            image = cv2.imread(str(img_path))
            
            if image is None:
                result['error'] = "Erro ao carregar imagem"
                return result
            
            # Verificar se deve salvar debug desta imagem
            save_debug = self.debug_images_count < self.max_debug_images
            
            # Processar com pipeline (com debug se necess√°rio)
            start_time = time.time()
            if save_debug:
                pipeline_result = self._process_with_debug(image, img_name)
                self.debug_images_count += 1
                logger.info(f"üñºÔ∏è  Imagens de debug salvas para: {img_name} ({self.debug_images_count}/{self.max_debug_images})")
            else:
                pipeline_result = self.pipeline.process(image)
            processing_time = time.time() - start_time
            
            result['processing_time'] = float(processing_time)
            result['success'] = True
            
            # Extrair informa√ß√µes das etapas
            # 1. Detec√ß√£o
            detections = pipeline_result.get('detections', [])
            result['num_detections'] = int(len(detections))
            result['detection_confidences'] = [float(d.get('confidence', 0)) for d in detections]
            result['avg_detection_confidence'] = float(np.mean(result['detection_confidences']) if result['detection_confidences'] else 0)
            
            # 2. OCR
            ocr_results = pipeline_result.get('ocr_results', [])
            result['ocr_texts'] = [str(ocr.get('text', '')) for ocr in ocr_results]
            result['ocr_confidences'] = [float(ocr.get('confidence', 0)) for ocr in ocr_results]
            
            # 3. Datas parseadas
            dates = pipeline_result.get('dates', [])
            result['num_dates_found'] = int(len(dates))
            result['dates'] = [str(d.get('date_str', '')) for d in dates]
            
            # 4. Melhor resultado (best_date)
            best_date = pipeline_result.get('best_date')
            if best_date:
                predicted_text = str(best_date.get('text', ''))
                predicted_date = str(best_date.get('date_str', ''))
                final_confidence = float(best_date.get('combined_confidence', 0))
            else:
                predicted_text = ''
                predicted_date = ''
                final_confidence = 0.0
            
            result['predicted_text'] = predicted_text
            result['predicted_date'] = predicted_date
            result['final_confidence'] = final_confidence
            
            # Calcular m√©tricas de compara√ß√£o (usar predicted_date ao inv√©s de predicted_text)
            metrics = self._calculate_comparison_metrics(
                predicted_date,
                ground_truth
            )
            result.update(metrics)
            
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"‚ùå Erro ao processar {img_name}: {e}")
        
        return result
    
    def _process_with_debug(
        self,
        image: np.ndarray,
        img_name: str
    ) -> Dict[str, Any]:
        """
        Processa uma imagem salvando imagens de debug intermedi√°rias.
        
        Salva:
        - input.jpg: Imagem original
        - crop.jpg: Regi√£o detectada pelo YOLO
        - preprocessamento.jpg: Ap√≥s pr√©-processamento
        - ocr.jpg: Resultado do OCR com anota√ß√µes
        
        Args:
            image: Imagem numpy array (BGR)
            img_name: Nome da imagem
            
        Returns:
            Resultado do processamento (igual ao pipeline.process())
        """
        import time

        # Criar diret√≥rio de debug para esta imagem
        safe_name = img_name.replace('.', '_').replace('/', '_')
        img_debug_dir = self.debug_output_dir / safe_name
        img_debug_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"üíæ Salvando imagens de debug em: {img_debug_dir}")
        
        # 1. Salvar input original
        input_path = img_debug_dir / "input.jpg"
        cv2.imwrite(str(input_path), image)
        logger.debug(f"   ‚úì Salvo: input.jpg")
        
        # Executar detec√ß√£o YOLO
        detections = self.pipeline._detect_regions(image)
        
        if not detections:
            logger.warning("‚ö†Ô∏è  Nenhuma regi√£o detectada - n√£o h√° crop para salvar")
            return {
                'success': False,
                'detections': [],
                'ocr_results': [],
                'dates': [],
                'best_date': None,
                'processing_time': 0,
                'error': 'No detections'
            }
        
        # Processar primeira detec√ß√£o (salvando debug)
        detection = detections[0]
        
        # 2. Extrair e salvar crop
        crop = self.pipeline._extract_crop(image, detection)
        crop_path = img_debug_dir / "crop.jpg"
        cv2.imwrite(str(crop_path), crop)
        logger.debug(f"   ‚úì Salvo: crop.jpg")
        
        # 3. Pr√©-processar e salvar
        if self.pipeline.preprocessor:
            crop_processed = self.pipeline.preprocessor.process(crop)
            preprocess_path = img_debug_dir / "preprocessamento.jpg"
            cv2.imwrite(str(preprocess_path), crop_processed)
            logger.debug(f"   ‚úì Salvo: preprocessamento.jpg")
        else:
            crop_processed = crop
            # Salvar mesmo sem pr√©-processamento
            preprocess_path = img_debug_dir / "preprocessamento.jpg"
            cv2.imwrite(str(preprocess_path), crop)
            logger.debug(f"   ‚úì Salvo: preprocessamento.jpg (sem pr√©-processamento)")
        
        # 4. Executar OCR
        text, confidence = self.pipeline.ocr_engine.extract_text(crop_processed)
        
        # Criar visualiza√ß√£o do OCR
        ocr_viz = crop_processed.copy()
        
        # Adicionar texto na imagem
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.8
        thickness = 2
        
        # Background para o texto
        text_label = f"OCR: '{text}'"
        conf_label = f"Conf: {confidence:.2%}"
        
        # Posi√ß√£o do texto
        y_offset = 30
        
        # Desenhar fundo branco para melhor legibilidade
        (text_w, text_h), _ = cv2.getTextSize(text_label, font, font_scale, thickness)
        cv2.rectangle(ocr_viz, (5, 5), (text_w + 15, y_offset + 10), (255, 255, 255), -1)
        
        # Desenhar texto
        cv2.putText(ocr_viz, text_label, (10, y_offset), font, font_scale, (0, 0, 255), thickness)
        
        # Adicionar confian√ßa
        y_offset += 35
        (conf_w, conf_h), _ = cv2.getTextSize(conf_label, font, font_scale, thickness)
        cv2.rectangle(ocr_viz, (5, y_offset - 25), (conf_w + 15, y_offset + 10), (255, 255, 255), -1)
        cv2.putText(ocr_viz, conf_label, (10, y_offset), font, font_scale, (0, 255, 0), thickness)
        
        ocr_path = img_debug_dir / "ocr.jpg"
        cv2.imwrite(str(ocr_path), ocr_viz)
        logger.debug(f"   ‚úì Salvo: ocr.jpg")
        
        # Agora executar o pipeline completo normalmente para obter todos os resultados
        pipeline_result = self.pipeline.process(image, image_name=img_name)
        
        logger.info(f"‚úÖ Debug completo salvo em: {img_debug_dir}")
        
        return pipeline_result
    
    def _normalize_date_format(self, date_str: str) -> str:
        """
        Normaliza formato de data para YYYY-MM-DD.
        
        Aceita:
        - DD/MM/YYYY
        - YYYY-MM-DD
        - DD-MM-YYYY
        
        Returns:
            Data no formato YYYY-MM-DD ou string vazia se inv√°lida
        """
        from datetime import datetime
        
        if not date_str:
            return ''
        
        # J√° est√° no formato correto
        if len(date_str) == 10 and date_str[4] == '-' and date_str[7] == '-':
            return date_str
        
        # Tentar parsear outros formatos
        formats = [
            '%d/%m/%Y',  # DD/MM/YYYY
            '%d-%m-%Y',  # DD-MM-YYYY
            '%Y/%m/%d',  # YYYY/MM/DD
        ]
        
        for fmt in formats:
            try:
                dt = datetime.strptime(date_str, fmt)
                return dt.strftime('%Y-%m-%d')
            except ValueError:
                continue
        
        # N√£o conseguiu parsear
        return date_str
    
    def _calculate_comparison_metrics(
        self,
        predicted: str,
        ground_truth: str
    ) -> Dict[str, float]:
        """
        Calcula m√©tricas de compara√ß√£o entre texto predito e ground truth.
        
        Args:
            predicted: Texto predito
            ground_truth: Texto esperado
            
        Returns:
            Dicion√°rio com m√©tricas
        """
        metrics = {}
        
        # Normalizar datas para o mesmo formato (YYYY-MM-DD)
        pred_normalized = self._normalize_date_format(predicted)
        gt_normalized = self._normalize_date_format(ground_truth)
        
        # Normalizar textos
        pred = pred_normalized.strip()
        gt = gt_normalized.strip()
        
        # 1. Exact Match
        metrics['exact_match'] = float(1.0 if pred == gt else 0.0)
        
        # 2. Similarity (SequenceMatcher) - usar datas normalizadas
        similarity = SequenceMatcher(None, pred, gt).ratio()
        metrics['similarity'] = float(similarity)
        
        # 3. Partial Match (>50% similaridade)
        metrics['partial_match'] = float(1.0 if similarity >= 0.5 else 0.0)
        
        # 4. Character Error Rate (CER)
        metrics['cer'] = float(self._calculate_cer(pred, gt))
        
        # 5. Word Error Rate (WER)
        metrics['wer'] = float(self._calculate_wer(pred, gt))
        
        return metrics
    
    def _calculate_cer(self, predicted: str, ground_truth: str) -> float:
        """Calcula Character Error Rate."""
        if not ground_truth:
            return 1.0 if predicted else 0.0
        
        # Levenshtein distance
        m, n = len(predicted), len(ground_truth)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if predicted[i-1] == ground_truth[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
        
        return dp[m][n] / n
    
    def _calculate_wer(self, predicted: str, ground_truth: str) -> float:
        """Calcula Word Error Rate."""
        pred_words = predicted.split()
        gt_words = ground_truth.split()
        
        if not gt_words:
            return 1.0 if pred_words else 0.0
        
        # Levenshtein distance para palavras
        m, n = len(pred_words), len(gt_words)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if pred_words[i-1] == gt_words[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
        
        return dp[m][n] / n
    
    def _calculate_metrics_summary(self) -> Dict[str, Any]:
        """Calcula m√©tricas agregadas."""
        if not self.results:
            return {}
        
        df = pd.DataFrame(self.results)
        
        summary = {}
        
        # M√©tricas gerais
        total = int(len(df))
        successful = int(df['success'].sum())
        
        summary['general'] = {
            'total_images': total,
            'successful': successful,
            'failed': total - successful,
            'success_rate': float((successful / total * 100) if total > 0 else 0)
        }
        
        # Filtrar apenas sucessos para m√©tricas
        df_success = df[df['success'] == True]
        
        if len(df_success) == 0:
            return summary
        
        # M√©tricas de detec√ß√£o
        if self.config['metrics']['detection']['enabled'] and 'num_detections' in df_success.columns:
            summary['detection'] = {
                'detection_rate': float((df_success['num_detections'] > 0).mean() * 100),
                'avg_detections_per_image': float(df_success['num_detections'].mean()),
                'avg_confidence': float(df_success['avg_detection_confidence'].mean()) if 'avg_detection_confidence' in df_success.columns else 0.0
            }
        
        # M√©tricas de OCR
        if self.config['metrics']['ocr']['enabled'] and 'exact_match' in df_success.columns:
            summary['ocr'] = {
                'exact_match_rate': float(df_success['exact_match'].mean() * 100),
                'partial_match_rate': float(df_success['partial_match'].mean() * 100) if 'partial_match' in df_success.columns else 0.0,
                'avg_similarity': float(df_success['similarity'].mean()) if 'similarity' in df_success.columns else 0.0,
                'avg_cer': float(df_success['cer'].mean()) if 'cer' in df_success.columns else 1.0,
                'avg_wer': float(df_success['wer'].mean()) if 'wer' in df_success.columns else 1.0
            }
        
        # M√©tricas de parsing de data
        if self.config['metrics']['date_parsing']['enabled'] and 'num_dates_found' in df_success.columns:
            summary['date_parsing'] = {
                'date_found_rate': float((df_success['num_dates_found'] > 0).mean() * 100),
                'avg_dates_per_image': float(df_success['num_dates_found'].mean())
            }
        
        # M√©tricas end-to-end
        if self.config['metrics']['end_to_end']['enabled']:
            summary['end_to_end'] = {
                'pipeline_accuracy': float(df_success['exact_match'].mean() * 100) if 'exact_match' in df_success.columns else 0.0,
                'avg_processing_time': float(df_success['processing_time'].mean()) if 'processing_time' in df_success.columns else 0.0,
                'avg_final_confidence': float(df_success['final_confidence'].mean()) if 'final_confidence' in df_success.columns else 0.0
            }
        
        return summary
    
    def _analyze_errors(self) -> Dict[str, Any]:
        """Analisa erros da pipeline."""
        df = pd.DataFrame(self.results)
        
        analysis = {
            'error_breakdown': {},
            'error_examples': []
        }
        
        # Classificar erros por etapa
        if self.config['error_analysis']['classify_by_stage']:
            # Erros de detec√ß√£o
            if 'num_detections' in df.columns:
                no_detections = df[df['num_detections'] == 0]
                analysis['error_breakdown']['no_detection'] = int(len(no_detections))
            
            # Erros de OCR
            if 'num_detections' in df.columns and 'exact_match' in df.columns:
                has_detection_no_match = df[(df['num_detections'] > 0) & (df['exact_match'] == 0)]
                analysis['error_breakdown']['detection_but_wrong_ocr'] = int(len(has_detection_no_match))
            
            # Erros de parsing
            has_ocr_no_date = df[(df['num_detections'] > 0) & (df['num_dates_found'] == 0)]
            analysis['error_breakdown']['ocr_but_no_date_parsed'] = int(len(has_ocr_no_date))
        
        # Salvar exemplos de erros
        if self.config['error_analysis']['save_error_examples']:
            max_examples = self.config['error_analysis']['max_error_examples']
            
            errors = df[df['exact_match'] == 0].head(max_examples)
            
            for _, row in errors.iterrows():
                analysis['error_examples'].append({
                    'image_name': row['image_name'],
                    'ground_truth': row['ground_truth'],
                    'predicted_text': row.get('predicted_text', ''),
                    'cer': row.get('cer', 1.0),
                    'similarity': row.get('similarity', 0.0)
                })
        
        return analysis
    
    def _convert_to_native_types(self, obj):
        """
        Converte recursivamente tipos numpy/pandas para tipos nativos Python.
        
        Args:
            obj: Objeto a converter
            
        Returns:
            Objeto com tipos nativos Python
        """
        if isinstance(obj, dict):
            return {key: self._convert_to_native_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_native_types(item) for item in obj]
        elif isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif pd.notna(obj) and pd.api.types.is_integer_dtype(type(obj)):
            return int(obj)
        elif pd.notna(obj) and pd.api.types.is_float_dtype(type(obj)):
            return float(obj)
        elif pd.isna(obj):
            return None
        else:
            return obj
    
    def save_results(self, output_dir: Path):
        """Salva resultados da avalia√ß√£o."""
        logger.info(f"\nüíæ Salvando resultados em: {output_dir}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. CSV detalhado
        if self.config['output']['save_results']['detailed_csv']:
            csv_path = output_dir / 'evaluation_results.csv'
            df = pd.DataFrame(self.results)
            df.to_csv(csv_path, index=False, encoding='utf-8')
            logger.info(f"   ‚úÖ CSV detalhado: {csv_path}")
        
        # 2. JSON completo (converter tipos antes de salvar)
        if self.config['output']['save_results']['full_json']:
            json_path = output_dir / 'evaluation_results.json'
            with open(json_path, 'w', encoding='utf-8') as f:
                json_data = {
                    'results': self._convert_to_native_types(self.results),
                    'metrics_summary': self._convert_to_native_types(self.metrics_summary),
                    'error_analysis': self._convert_to_native_types(self.error_analysis)
                }
                json.dump(json_data, f, indent=2, ensure_ascii=False)
            logger.info(f"   ‚úÖ JSON completo: {json_path}")
        
        # 3. M√©tricas agregadas (converter tipos antes de salvar)
        if self.config['output']['save_results']['metrics_summary']:
            metrics_path = output_dir / 'metrics_summary.json'
            with open(metrics_path, 'w', encoding='utf-8') as f:
                metrics_data = self._convert_to_native_types(self.metrics_summary)
                json.dump(metrics_data, f, indent=2, ensure_ascii=False)
            logger.info(f"   ‚úÖ M√©tricas agregadas: {metrics_path}")
        
        # 4. Relat√≥rio markdown
        if self.config['output']['save_results']['markdown_report']:
            report_path = output_dir / 'evaluation_report.md'
            self._generate_markdown_report(report_path)
            logger.info(f"   ‚úÖ Relat√≥rio markdown: {report_path}")
    
    def save_visualizations(self, output_dir: Path):
        """Gera e salva visualiza√ß√µes."""
        if not self.config['output']['save_visualizations']['metrics_plots']:
            return
        
        logger.info(f"\nüìä Gerando visualiza√ß√µes...")
        viz_dir = output_dir / 'visualizations'
        viz_dir.mkdir(parents=True, exist_ok=True)
        
        df = pd.DataFrame(self.results)
        df_success = df[df['success'] == True]
        
        if len(df_success) == 0:
            logger.warning("‚ö†Ô∏è Nenhum resultado bem-sucedido para visualizar")
            return
        
        # Configurar estilo
        sns.set_style("whitegrid")
        plt.rcParams['figure.figsize'] = (16, 12)
        
        # 1. M√©tricas principais
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # Exact Match Rate
        ax = axes[0, 0]
        exact_rate = df_success['exact_match'].mean() * 100
        partial_rate = df_success['partial_match'].mean() * 100
        categories = ['Exact Match', 'Partial Match', 'No Match']
        values = [exact_rate, partial_rate - exact_rate, 100 - partial_rate]
        colors = ['#2ecc71', '#f39c12', '#e74c3c']
        ax.bar(categories, values, color=colors, edgecolor='black')
        ax.set_ylabel('Porcentagem (%)')
        ax.set_title('Taxa de Match', fontsize=14, fontweight='bold')
        ax.set_ylim(0, 100)
        for i, v in enumerate(values):
            ax.text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')
        
        # CER Distribution
        ax = axes[0, 1]
        df_success['cer'].hist(bins=20, ax=ax, edgecolor='black', color='skyblue')
        ax.axvline(df_success['cer'].mean(), color='r', linestyle='--',
                   label=f'M√©dia: {df_success["cer"].mean():.3f}')
        ax.set_xlabel('Character Error Rate')
        ax.set_ylabel('Frequ√™ncia')
        ax.set_title('Distribui√ß√£o de CER', fontsize=14, fontweight='bold')
        ax.legend()
        
        # Processing Time
        ax = axes[0, 2]
        (df_success['processing_time'] * 1000).hist(bins=20, ax=ax, edgecolor='black', color='coral')
        ax.axvline(df_success['processing_time'].mean() * 1000, color='r', linestyle='--',
                   label=f'M√©dia: {df_success["processing_time"].mean()*1000:.1f}ms')
        ax.set_xlabel('Tempo (ms)')
        ax.set_ylabel('Frequ√™ncia')
        ax.set_title('Tempo de Processamento', fontsize=14, fontweight='bold')
        ax.legend()
        
        # Detection Confidence
        ax = axes[1, 0]
        df_success['avg_detection_confidence'].hist(bins=20, ax=ax, edgecolor='black', color='mediumseagreen')
        ax.set_xlabel('Confian√ßa')
        ax.set_ylabel('Frequ√™ncia')
        ax.set_title('Confian√ßa das Detec√ß√µes', fontsize=14, fontweight='bold')
        
        # Similarity Distribution
        ax = axes[1, 1]
        df_success['similarity'].hist(bins=20, ax=ax, edgecolor='black', color='mediumpurple')
        ax.set_xlabel('Similaridade')
        ax.set_ylabel('Frequ√™ncia')
        ax.set_title('Distribui√ß√£o de Similaridade', fontsize=14, fontweight='bold')
        
        # Error Breakdown
        ax = axes[1, 2]
        if self.error_analysis.get('error_breakdown'):
            breakdown = self.error_analysis['error_breakdown']
            labels = list(breakdown.keys())
            values = list(breakdown.values())
            ax.bar(range(len(labels)), values, color='indianred', edgecolor='black')
            ax.set_xticks(range(len(labels)))
            ax.set_xticklabels(labels, rotation=45, ha='right')
            ax.set_ylabel('Quantidade')
            ax.set_title('Breakdown de Erros', fontsize=14, fontweight='bold')
            for i, v in enumerate(values):
                ax.text(i, v + 0.5, str(v), ha='center', fontweight='bold')
        
        plt.tight_layout()
        plot_path = viz_dir / 'metrics_overview.png'
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        logger.info(f"   ‚úÖ Gr√°ficos salvos: {viz_dir}")
    
    def _generate_markdown_report(self, output_path: Path):
        """Gera relat√≥rio markdown."""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# üìä Relat√≥rio de Avalia√ß√£o da Pipeline\n\n")
            f.write(f"**Data:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # M√©tricas gerais
            f.write("## üìà M√©tricas Gerais\n\n")
            general = self.metrics_summary.get('general', {})
            f.write(f"- **Total de Imagens:** {general.get('total_images', 0)}\n")
            f.write(f"- **Processadas com Sucesso:** {general.get('successful', 0)}\n")
            f.write(f"- **Falhas:** {general.get('failed', 0)}\n")
            f.write(f"- **Taxa de Sucesso:** {general.get('success_rate', 0):.2f}%\n\n")
            
            # M√©tricas de detec√ß√£o
            if 'detection' in self.metrics_summary:
                f.write("## üéØ M√©tricas de Detec√ß√£o (YOLO)\n\n")
                detection = self.metrics_summary['detection']
                f.write(f"- **Taxa de Detec√ß√£o:** {detection.get('detection_rate', 0):.2f}%\n")
                f.write(f"- **M√©dia de Detec√ß√µes/Imagem:** {detection.get('avg_detections_per_image', 0):.2f}\n")
                f.write(f"- **Confian√ßa M√©dia:** {detection.get('avg_confidence', 0):.3f}\n\n")
            
            # M√©tricas de OCR
            if 'ocr' in self.metrics_summary:
                f.write("## üî§ M√©tricas de OCR\n\n")
                ocr = self.metrics_summary['ocr']
                f.write(f"- **Exact Match:** {ocr.get('exact_match_rate', 0):.2f}%\n")
                f.write(f"- **Partial Match:** {ocr.get('partial_match_rate', 0):.2f}%\n")
                f.write(f"- **Similaridade M√©dia:** {ocr.get('avg_similarity', 0):.3f}\n")
                f.write(f"- **CER M√©dio:** {ocr.get('avg_cer', 0):.3f}\n")
                f.write(f"- **WER M√©dio:** {ocr.get('avg_wer', 0):.3f}\n\n")
            
            # M√©tricas de parsing
            if 'date_parsing' in self.metrics_summary:
                f.write("## üìÖ M√©tricas de Parsing de Datas\n\n")
                parsing = self.metrics_summary['date_parsing']
                f.write(f"- **Taxa de Datas Encontradas:** {parsing.get('date_found_rate', 0):.2f}%\n")
                f.write(f"- **M√©dia de Datas/Imagem:** {parsing.get('avg_dates_per_image', 0):.2f}\n\n")
            
            # M√©tricas end-to-end
            if 'end_to_end' in self.metrics_summary:
                f.write("## üéØ M√©tricas End-to-End\n\n")
                e2e = self.metrics_summary['end_to_end']
                f.write(f"- **Acur√°cia da Pipeline:** {e2e.get('pipeline_accuracy', 0):.2f}%\n")
                f.write(f"- **Tempo M√©dio de Processamento:** {e2e.get('avg_processing_time', 0):.3f}s\n")
                f.write(f"- **Confian√ßa Final M√©dia:** {e2e.get('avg_final_confidence', 0):.3f}\n\n")
            
            # An√°lise de erros
            if self.error_analysis:
                f.write("## üîç An√°lise de Erros\n\n")
                
                if 'error_breakdown' in self.error_analysis:
                    f.write("### Breakdown por Etapa\n\n")
                    for error_type, count in self.error_analysis['error_breakdown'].items():
                        f.write(f"- **{error_type}:** {count}\n")
                    f.write("\n")
                
                if 'error_examples' in self.error_analysis and self.error_analysis['error_examples']:
                    f.write("### Exemplos de Erros\n\n")
                    for i, example in enumerate(self.error_analysis['error_examples'][:5], 1):
                        f.write(f"**{i}. {example['image_name']}**\n\n")
                        f.write(f"- **Ground Truth:** `{example['ground_truth']}`\n")
                        f.write(f"- **Predi√ß√£o:** `{example['predicted_text']}`\n")
                        f.write(f"- **CER:** {example['cer']:.3f}\n")
                        f.write(f"- **Similaridade:** {example['similarity']:.3f}\n\n")


def load_config(config_path: str, cli_args: argparse.Namespace) -> Dict[str, Any]:
    """
    Carrega configura√ß√£o e sobrescreve com argumentos CLI.
    
    Args:
        config_path: Caminho do arquivo de configura√ß√£o
        cli_args: Argumentos da linha de comando
        
    Returns:
        Configura√ß√£o mesclada
    """
    # Carregar YAML
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # Sobrescrever com CLI args
    if cli_args.num_images is not None:
        config['dataset']['num_images'] = cli_args.num_images
    
    if cli_args.selection_mode is not None:
        config['dataset']['selection_mode'] = cli_args.selection_mode
    
    if cli_args.output is not None:
        config['output']['base_dir'] = cli_args.output
    
    return config


def main():
    parser = argparse.ArgumentParser(
        description="Avalia√ß√£o completa da pipeline end-to-end"
    )
    parser.add_argument(
        '--config',
        type=str,
        default='config/pipeline/pipeline_evaluation.yaml',
        help='Arquivo de configura√ß√£o da avalia√ß√£o'
    )
    parser.add_argument(
        '--num-images',
        type=int,
        help='N√∫mero de imagens para avaliar (sobrescreve config)'
    )
    parser.add_argument(
        '--selection-mode',
        type=str,
        choices=['all', 'first', 'random'],
        help='Modo de sele√ß√£o de imagens (sobrescreve config)'
    )
    parser.add_argument(
        '--output',
        type=str,
        help='Diret√≥rio de sa√≠da (sobrescreve config)'
    )
    
    args = parser.parse_args()
    
    # Carregar configura√ß√£o
    config = load_config(args.config, args)
    
    # Criar diret√≥rio de output
    output_dir = Path(config['output']['base_dir'])
    if config['output'].get('use_timestamp', True):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = output_dir / timestamp
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Configurar logging
    if config['logging']['save_to_file']:
        log_file = output_dir / 'evaluation.log'
        logger.add(log_file, level=config['logging']['level'])
    
    # Criar avaliador
    evaluator = PipelineEvaluator(config)
    
    # Configurar diret√≥rio de debug
    evaluator.debug_output_dir = output_dir / "debug_images"
    evaluator.debug_output_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"üìÅ Diret√≥rio de debug: {evaluator.debug_output_dir}")
    
    # Inicializar pipeline
    evaluator.initialize_pipeline()
    
    # Executar avalia√ß√£o
    start_time = time.time()
    results = evaluator.evaluate()
    total_time = time.time() - start_time
    
    # Salvar resultados
    evaluator.save_results(output_dir)
    
    # Salvar visualiza√ß√µes
    evaluator.save_visualizations(output_dir)
    
    # Relat√≥rio final
    logger.info("\n" + "=" * 80)
    logger.info("‚úÖ AVALIA√á√ÉO CONCLU√çDA")
    logger.info("=" * 80)
    logger.info(f"‚è±Ô∏è  Tempo total: {total_time:.2f}s")
    logger.info(f"üìÅ Resultados salvos em: {output_dir}")
    logger.info("=" * 80)
    
    # Mostrar m√©tricas principais
    if evaluator.metrics_summary:
        logger.info("\nüìä Resumo das M√©tricas:")
        
        if 'general' in evaluator.metrics_summary:
            general = evaluator.metrics_summary['general']
            logger.info(f"   Total: {general['total_images']} | Sucesso: {general['success_rate']:.1f}%")
        
        if 'ocr' in evaluator.metrics_summary:
            ocr = evaluator.metrics_summary['ocr']
            logger.info(f"   Exact Match: {ocr['exact_match_rate']:.1f}% | CER: {ocr['avg_cer']:.3f}")
        
        if 'end_to_end' in evaluator.metrics_summary:
            e2e = evaluator.metrics_summary['end_to_end']
            logger.info(f"   Pipeline Accuracy: {e2e['pipeline_accuracy']:.1f}% | Tempo: {e2e['avg_processing_time']:.3f}s")


if __name__ == '__main__':
    main()
