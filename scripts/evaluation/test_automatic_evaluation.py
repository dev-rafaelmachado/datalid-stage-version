#!/usr/bin/env python3
"""
ğŸ§ª Script de teste rÃ¡pido para validar avaliaÃ§Ã£o automÃ¡tica

Testa a funcionalidade de avaliaÃ§Ã£o automÃ¡tica no conjunto de teste.
"""

import sys
from pathlib import Path

# Adicionar src ao path
SCRIPT_DIR = Path(__file__).resolve().parent
ROOT_DIR = SCRIPT_DIR.parent.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from loguru import logger

from src.yolo import TrainingConfig, YOLOConfig, YOLOTrainer


def test_automatic_evaluation():
    """Testa avaliaÃ§Ã£o automÃ¡tica apÃ³s treinamento."""
    logger.info("ğŸ§ª Testando avaliaÃ§Ã£o automÃ¡tica no conjunto de teste")
    logger.info("=" * 70)
    
    # Configurar para treinamento rÃ¡pido (apenas para teste)
    config = YOLOConfig()
    config.training = TrainingConfig(
        model='yolov8n-seg.pt',  # Modelo pequeno
        epochs=2,  # Apenas 2 Ã©pocas para teste rÃ¡pido
        batch=4,
        imgsz=320,  # Imagem menor para velocidade
        patience=50,
        workers=2
    )
    
    # Criar trainer
    trainer = YOLOTrainer(config_obj=config)
    
    # Dataset
    data_path = ROOT_DIR / 'data' / 'processed' / 'v1_segment'
    
    if not data_path.exists():
        logger.error(f"âŒ Dataset nÃ£o encontrado: {data_path}")
        logger.info("ğŸ’¡ Execute prepare_dataset.py primeiro")
        return False
    
    try:
        logger.info("ğŸ‹ï¸ Iniciando treinamento de teste (2 Ã©pocas)...")
        
        # Treinar COM teste automÃ¡tico
        metrics = trainer.train(
            data_path=data_path,
            test_after_training=True,  # Ativar teste automÃ¡tico
            project='experiments',
            name='test_auto_eval'
        )
        
        # Verificar se teste foi executado
        if metrics.test_results:
            logger.success("âœ… Teste automÃ¡tico executado com sucesso!")
            logger.info(f"ğŸ“Š MÃ©tricas de teste: {metrics.test_results}")
            
            # Verificar arquivo test_results.json
            test_results_path = ROOT_DIR / 'experiments' / 'test_auto_eval' / 'test_results.json'
            if test_results_path.exists():
                logger.success(f"âœ… Arquivo test_results.json criado: {test_results_path}")
            else:
                logger.warning("âš ï¸ Arquivo test_results.json nÃ£o encontrado")
            
            return True
        else:
            logger.error("âŒ Teste automÃ¡tico nÃ£o foi executado")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Erro durante teste: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def test_disable_automatic_evaluation():
    """Testa desabilitar avaliaÃ§Ã£o automÃ¡tica."""
    logger.info("\nğŸ§ª Testando DESABILITAR avaliaÃ§Ã£o automÃ¡tica")
    logger.info("=" * 70)
    
    # Configurar para treinamento rÃ¡pido
    config = YOLOConfig()
    config.training = TrainingConfig(
        model='yolov8n-seg.pt',
        epochs=1,
        batch=4,
        imgsz=320,
        patience=50,
        workers=2
    )
    
    trainer = YOLOTrainer(config_obj=config)
    data_path = ROOT_DIR / 'data' / 'processed' / 'v1_segment'
    
    if not data_path.exists():
        logger.error(f"âŒ Dataset nÃ£o encontrado: {data_path}")
        return False
    
    try:
        logger.info("ğŸ‹ï¸ Iniciando treinamento sem teste automÃ¡tico...")
        
        # Treinar SEM teste automÃ¡tico
        metrics = trainer.train(
            data_path=data_path,
            test_after_training=False,  # Desabilitar teste automÃ¡tico
            project='experiments',
            name='test_no_auto_eval'
        )
        
        # Verificar que teste NÃƒO foi executado
        if metrics.test_results is None:
            logger.success("âœ… Teste automÃ¡tico corretamente desabilitado!")
            return True
        else:
            logger.error("âŒ Teste foi executado mesmo com test_after_training=False")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Erro durante teste: {str(e)}")
        return False


def main():
    """FunÃ§Ã£o principal."""
    logger.info("ğŸš€ TESTE DE AVALIAÃ‡ÃƒO AUTOMÃTICA")
    logger.info("=" * 70)
    logger.info("Este script testa a funcionalidade de avaliaÃ§Ã£o automÃ¡tica")
    logger.info("SerÃ¡ executado um treinamento curto (2 Ã©pocas) para validar")
    logger.info("=" * 70)
    logger.info("")
    
    # Teste 1: Com avaliaÃ§Ã£o automÃ¡tica
    test1_passed = test_automatic_evaluation()
    
    # Teste 2: Sem avaliaÃ§Ã£o automÃ¡tica
    test2_passed = test_disable_automatic_evaluation()
    
    # Resultado final
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š RESULTADOS DOS TESTES")
    logger.info("=" * 70)
    logger.info(f"Teste 1 (com avaliaÃ§Ã£o): {'âœ… PASSOU' if test1_passed else 'âŒ FALHOU'}")
    logger.info(f"Teste 2 (sem avaliaÃ§Ã£o): {'âœ… PASSOU' if test2_passed else 'âŒ FALHOU'}")
    logger.info("=" * 70)
    
    if test1_passed and test2_passed:
        logger.success("ğŸ‰ TODOS OS TESTES PASSARAM!")
        return 0
    else:
        logger.error("âŒ ALGUNS TESTES FALHARAM")
        return 1


if __name__ == "__main__":
    sys.exit(main())
